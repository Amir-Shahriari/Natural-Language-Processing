{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gg5PvL45XT_e"
   },
   "source": [
    "# COMP8420 Advanced Natural Language Processing\n",
    "## Week 1 - Setting Up the Environment, Data Pre-processing & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZaJVK7azikk"
   },
   "source": [
    "## Today's Topics\n",
    "- Jupyter Notebook\n",
    "- Environment & Dependencies\n",
    "- Text Data Pre-processing\n",
    "- Classification & Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzdd4Nd4dUEF"
   },
   "source": [
    "### 1. Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNv4nZW9m9_C"
   },
   "source": [
    "[Jupyter Notebook](https://jupyter.org/) is an interactive development environment that supports **real-time code execution and text presentation**.  \n",
    "\n",
    "Due to its convenience, ease of use for running demos, and efficiency in obtaining results, it is widely used in the machine learning community.  \n",
    "\n",
    "For our workshops, we will use **Google Colab**, a cloud-based implementation of Jupyter Notebook. Alternatively, Jupyter Notebook can be installed locally by following the instructions at [Jupyter's official site](https://jupyter.org/install)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FyZXApzt0w9"
   },
   "source": [
    "#### Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP5b-NTbt_Cj"
   },
   "source": [
    "Notebooks consist of two types of cells: *text cells* and *code cells*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEJwfixvytSY"
   },
   "source": [
    "**Text cells**: This cell is an example of a text cell. It supports markup languages such as Markdown, HTML, and LaTeX, allowing you to format plain text, add [hyperlinks](https://www.markdownguide.org/getting-started/), display equations (e.g., $y = wx + b$), create tables, insert images, and more.  \n",
    "\n",
    "You can edit a text cell by *double-clicking* on it. Press `<Shift> + <Enter>`  to render the formatted content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhaB0R25y6xa"
   },
   "source": [
    "**Code cells**: Jupyter Notebook supports multiple programming languages, referred to as **kernels** in Jupyter terminology. In this course, we will primarily use **Python 3**, as it is widely adopted in the machine learning and data science communities.  \n",
    "\n",
    "The following cell is an example of a code cell. You can execute it in several ways:  \n",
    "- Click the *Run* â–¶ button (visible when you hover over the cell).  \n",
    "- Press `<Shift> + <Enter>` to run the cell and move to the next one.  \n",
    "- Press `<Ctrl> + <Enter>` to run the cell without advancing.  \n",
    "\n",
    "To execute all cells at once, select *Run all* from the *Runtime* menu in the top right of the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DHKo1_5hdYcR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************\n",
      "*  Welcome to COMP8420 Advanced Natural Language Processing!  *\n",
      "***************************************************************\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Print messages\"\"\"\n",
    "\n",
    "message = \"Welcome to COMP8420 Advanced Natural Language Processing!\"\n",
    "border = \"*\" * (len(message) + 6)\n",
    "\n",
    "# Print the formatted message\n",
    "print(border)\n",
    "print(f\"*  {message}  *\")\n",
    "print(border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/amir/miniconda3/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/amir/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/amir/miniconda3/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/amir/miniconda3/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/amir/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/amir/miniconda3/lib/python3.12/site-packages (from matplotlib) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/amir/miniconda3/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/amir/miniconda3/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/amir/miniconda3/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/amir/miniconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/amir/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vTx1evcxMzBt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Interactively visualize data points\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#### Parameters you can easily change ####\n",
    "center = 4\n",
    "width = 2\n",
    "view_range = 8\n",
    "##########################################\n",
    "\n",
    "# Create the points to plot\n",
    "x = np.arange(center - view_range, center + view_range, 0.01)\n",
    "y = width * (x - center)**2\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(x, y, label=f\"y = {width}(x - {center})$^2$\", color='blue', linestyle='-')\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.title(\"Parabola\")\n",
    "\n",
    "#### What else configs can be set for the plot? ####\n",
    "## Q1. How to add grid lines?\n",
    "\n",
    "\n",
    "## Q2. How to set a legend?\n",
    "\n",
    "####################################################\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRM3XZCXahYd"
   },
   "source": [
    "### 2. Environment & Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcNJc3oxfojj"
   },
   "source": [
    "A machine learning development environment is your complete workspace setup, primarily consisting of:\n",
    "- Python distribution (e.g., Python 2/3, and specific versions like Python 3.10)\n",
    "- Essential dependency packages (e.g., NumPy, Pandas, scikit-learn)  \n",
    "\n",
    "An isolated environment enables you to manage dependencies needed for running program, without writing every function from scratch.\n",
    "\n",
    "Google Colab has an embedded environment, which installs most widely used packages by default.\n",
    "\n",
    "By running the cell below, you can verify whether some basic packages are installed and what their installed versions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IPORnf8LqxQy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0]\n",
      "PyTorch version: 2.6.0+cu124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d2f057923f4ec380a15b9d6dcfda67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.39.3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Verify installation and version of pakcages\"\"\"\n",
    "\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch is not installed!\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"Transformers version: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Transformers is not installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRxZRb9YsTrd"
   },
   "source": [
    "If any additional packages need to be installed, use `!pip install <package_name>` to run a shell command within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.39.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xvhOgC1xqMjK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /home/amir/miniconda3/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/amir/miniconda3/lib/python3.12/site-packages (from evaluate) (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/amir/miniconda3/lib/python3.12/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: dill in /home/amir/miniconda3/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/amir/miniconda3/lib/python3.12/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/amir/miniconda3/lib/python3.12/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/amir/miniconda3/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/amir/miniconda3/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/amir/miniconda3/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/amir/miniconda3/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/amir/miniconda3/lib/python3.12/site-packages (from evaluate) (0.24.6)\n",
      "Requirement already satisfied: packaging in /home/amir/miniconda3/lib/python3.12/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /home/amir/miniconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/amir/miniconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /home/amir/miniconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.11.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/amir/miniconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/amir/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/amir/miniconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/amir/miniconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/amir/miniconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/amir/miniconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/amir/miniconda3/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/amir/miniconda3/lib/python3.12/site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/amir/miniconda3/lib/python3.12/site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/amir/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/amir/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/amir/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/amir/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/amir/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/amir/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/amir/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/amir/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mJkqgDnton7"
   },
   "source": [
    "As a free cloud service, Google Colab **will not** preserve the state of your custom-installed packages. Therefore, it is also a good practice to install development environments on your own computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zl6nCpLblskR"
   },
   "source": [
    "#### Anaconda (Python + ML Package Manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN2y-Sn6u625"
   },
   "source": [
    "[Anaconda Distribution](https://www.anaconda.com/) provides an environment manager called *conda*, and is still one of the most widely adopted tool for machine learning practitioners to manage their environments and dependencies.\n",
    "\n",
    "It can be downloaded from its [official website](https://www.anaconda.com/download/success). Two versions are available:\n",
    "- Anaconda: Full distribution with pre-installed data science packages\n",
    "- Miniconda: Lightweight version with only conda and its dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N28lyGv_0iH-"
   },
   "source": [
    "You can then create your own working environment and install packages after installing Anaconda or Miniconda.\n",
    "\n",
    "Here are example command line codes to set up the env in your local computer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck2dvCKC1xdj"
   },
   "source": [
    "\n",
    "- Create and activate new environment\n",
    "```bash\n",
    "conda create -n nlp-env python=3.10\n",
    "conda activate nlp-env  \n",
    "```\n",
    "\n",
    "- Install core data science packages  \n",
    "```bash\n",
    "conda install numpy pandas matplotlib scikit-learn jupyter notebook\n",
    "```\n",
    "\n",
    "- Install NLP packages, PyTorch and Hugging Face packages   \n",
    "```bash\n",
    "pip install nltk spacy gensim  \n",
    "pip install torch torchvision torchaudio\n",
    "pip install transformers datasets evaluate sentencepiece\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TW8YKvcA1Kw0"
   },
   "source": [
    "**Let's have a practice, please:**\n",
    "\n",
    ">- Step 1: Download and install Anaconda distribution\n",
    ">- Step 2: Create an environment (either from anaconda interface or terminal)\n",
    ">- Step 3: Install a package, e.g., Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqzASjprpI7d"
   },
   "source": [
    "> **ðŸ’¡ (Optional) Think About It:** To ensure project reproducibility:\n",
    "> - How can you export your environment configuration? (Hint: Try searching `conda env export` or `pip freeze`)\n",
    "> - What's the difference between requirements.txt and environment.yml?\n",
    "> - How do you recreate an environment from these files?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q81HTOeXgXM5"
   },
   "source": [
    "### 3. Text Data Pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6xf6pm9KgBw"
   },
   "source": [
    "Text data, often collected from the web, is typically **unstructured** and contains elements like HTML symbols that make it challenging to analyze directly. Therefore, data preprocessing is a crucial first step in any text processing task.\n",
    "\n",
    "In this workshop, we'll explore preprocessing techniques using HTML-formatted data collected from the subject's iLearn webpage.\n",
    "\n",
    "This hands-on example will demonstrate common cleaning and transformation steps needed to prepare web text for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ziRtyFc6KsYk"
   },
   "outputs": [],
   "source": [
    "corpus = '''\n",
    "<div class=\"summary\"><div class=\"no-overflow\"><p dir=\"ltr\" style=\"text-align: left;\"></p><strong>\n",
    "Welcome to COMP8420 Advanced Natural Language Processing!<br></strong><br>\n",
    "The last few years have seen very rapid improvements in the ability of computers to understand\n",
    "spoken and written text, and to create and translate. A lot of this improvement has come from one innovation:\n",
    "Transformers. In this unit we will deep-dive on to the Transformer architecture, and cover applications of\n",
    "Transformers (and other techniques) in industry and beyond.<br><br><br></div></div>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOmkM2qRKxF1"
   },
   "source": [
    "Those HTML-formatted symbols could be irrelevant to the text processing task, so we can use a [Regular Expression (Regex)](https://www.geeksforgeeks.org/python-substituting-patterns-in-text-using-regex/) to remove them and obtain a cleaner version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7eAH-h2EKuWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Welcome to COMP8420 Advanced Natural Language Processing! The last few years have seen very rapid improvements in the ability of computers to understand spoken and written text, and to create and translate. A lot of this improvement has come from one innovation: Transformers. In this unit we will deep-dive on to the Transformer architecture, and cover applications of Transformers (and other techniques) in industry and beyond.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "## Syntax of RegEx(Regular expression): re.sub(pattern, replacement, string)\n",
    "pattern=\"<[^>]+>\"\n",
    "document = re.sub(pattern, \"\", corpus)\n",
    "document = \" \".join(document.split())\n",
    "\n",
    "print('\\n',document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQ5iyThcLTZA"
   },
   "source": [
    "There is also a famous tool named [Natural Language Toolkit (NLTK)](https://www.nltk.org/) that is frequently used to conduct text data pre-processing, including sentence segmentation, tokenization, removing stop words, lemmatization and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/amir/miniconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/amir/miniconda3/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/amir/miniconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/amir/miniconda3/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/amir/miniconda3/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "g86QhgvpLSm5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/amir/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt_tab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WXmP0stjNGax"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to COMP8420 Advanced Natural Language Processing!\n",
      "The last few years have seen very rapid improvements in the ability of computers to understand spoken and written text, and to create and translate.\n",
      "A lot of this improvement has come from one innovation: Transformers.\n",
      "In this unit we will deep-dive on to the Transformer architecture, and cover applications of Transformers (and other techniques) in industry and beyond.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Sentence segmentation\"\"\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(document)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqRUIhCtP7mX"
   },
   "source": [
    "We can also use regular expression to split the paragraph into sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lZHKK3XQOXaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to COMP8420 Advanced Natural Language Processing\n",
      " The last few years have seen very rapid improvements in the ability of computers to understand spoken and written text, and to create and translate\n",
      " A lot of this improvement has come from one innovation: Transformers\n",
      " In this unit we will deep-dive on to the Transformer architecture, and cover applications of Transformers (and other techniques) in industry and beyond\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simple_sentences = re.split(r'[.!?]', document)\n",
    "\n",
    "for sentence in simple_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpXtMLJLQyrz"
   },
   "source": [
    "What could be the problem to use hardcoded regular expression rules to split the sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "koBjBPaBLr5k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Dr. Smith went to Washington D.C. Yesterday was his birthday.\"\n",
    "\n",
    "## Question: can you think of any other examples?\n",
    "text = \" \"\n",
    "\n",
    "regex_sentences = re.split(r'[.!?]', text)\n",
    "nltk_sentences = sent_tokenize(text)\n",
    "\n",
    "print(regex_sentences)\n",
    "print(nltk_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upSKkTOeSEUN"
   },
   "source": [
    "Text data are generally processed at the token level, so we should continue to chunk the sentences into a fine-grained level, using techniques called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rBXwmJMqSFk2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'unit', 'we', 'will', 'deep-dive', 'on', 'to', 'the', 'Transformer', 'architecture,', 'and', 'cover', 'applications', 'of', 'Transformers', '(and', 'other', 'techniques)', 'in', 'industry', 'and', 'beyond.']\n",
      "['In', 'this', 'unit', 'we', 'will', 'deep-dive', 'on', 'to', 'the', 'Transformer', 'architecture', ',', 'and', 'cover', 'applications', 'of', 'Transformers', '(', 'and', 'other', 'techniques', ')', 'in', 'industry', 'and', 'beyond', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tokenization\"\"\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_sentence = word_tokenize(sentences[3])\n",
    "\n",
    "print(sentences[3].split(\" \"))\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OravL29RYo3g"
   },
   "source": [
    "Similary, we can have more examples to check the differences between direct split, and the NLTK tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-BPNxXlSYh_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"don't\", 'want', 'to', 'go.', \"It's\", 'too', 'late.']\n",
      "['I', 'do', \"n't\", 'want', 'to', 'go', '.', 'It', \"'s\", 'too', 'late', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I don't want to go. It's too late.\"\n",
    "\n",
    "# split\n",
    "print(text.split())\n",
    "\n",
    "# NLTK word_tokenize\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WuKFRHK9ZAvU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'IN'), ('this', 'DT'), ('unit', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('deep-dive', 'VB'), ('on', 'IN'), ('to', 'TO'), ('the', 'DT'), ('Transformer', 'NNP'), ('architecture', 'NN'), (',', ','), ('and', 'CC'), ('cover', 'NN'), ('applications', 'NNS'), ('of', 'IN'), ('Transformers', 'NNP'), ('(', '('), ('and', 'CC'), ('other', 'JJ'), ('techniques', 'NNS'), (')', ')'), ('in', 'IN'), ('industry', 'NN'), ('and', 'CC'), ('beyond', 'IN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/amir/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "part_of_speech_tag = nltk.pos_tag(tokenized_sentence)\n",
    "print(part_of_speech_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ENqbIBzZ3U_"
   },
   "source": [
    "You can find more details of part-of-speech tags in [Penn TreeBank project](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and also [NLTK tutorial](https://www.nltk.org/book/ch05.html).\n",
    "\n",
    "Additionally, classical NLP techniques studied to remove 'not-so-important' tokens, such as stop words ('in', 'the' etc.), from the sentence to keep focus on key features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Vtuait9vazZK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/amir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/amir/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['In', 'this', 'unit', 'we', 'will', 'deep-dive', 'on', 'to', 'the', 'Transformer', 'architecture', ',', 'and', 'cover', 'applications', 'of', 'Transformers', '(', 'and', 'other', 'techniques', ')', 'in', 'industry', 'and', 'beyond', '.']\n",
      "After removing stop words: ['unit', 'deep-dive', 'Transformer', 'architecture', ',', 'cover', 'applications', 'Transformers', '(', 'techniques', ')', 'industry', 'beyond', '.']\n",
      "After lemmatization: ['unit', 'deep-dive', 'Transformer', 'architecture', ',', 'cover', 'application', 'Transformers', '(', 'technique', ')', 'industry', 'beyond', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Removing stop-words and lemmatization\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [word for word in tokenized_sentence if word.lower() not in stop_words]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "print(\"Original:\", tokenized_sentence)\n",
    "print(\"After removing stop words:\", filtered_tokens)\n",
    "print(\"After lemmatization:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epeNRHXvecyY"
   },
   "source": [
    "#### **Tokens are still in text format; how can we transform them into numerical representations, which can be used for downstream tasks?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lpPQN50djQdi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentences: ['Welcome to COMP8420 Advanced Natural Language Processing!', 'The last few years have seen very rapid improvements in the ability of computers to understand spoken and written text, and to create and translate.', 'A lot of this improvement has come from one innovation: Transformers.', 'In this unit we will deep-dive on to the Transformer architecture, and cover applications of Transformers (and other techniques) in industry and beyond.'] \n",
      "\n",
      "Feature names (vocabulary):\n",
      " ['ability' 'advanced' 'and' 'applications' 'architecture' 'beyond' 'come'\n",
      " 'comp8420' 'computers' 'cover' 'create' 'deep' 'dive' 'few' 'from' 'has'\n",
      " 'have' 'improvement' 'improvements' 'in' 'industry' 'innovation'\n",
      " 'language' 'last' 'lot' 'natural' 'of' 'on' 'one' 'other' 'processing'\n",
      " 'rapid' 'seen' 'spoken' 'techniques' 'text' 'the' 'this' 'to'\n",
      " 'transformer' 'transformers' 'translate' 'understand' 'unit' 'very' 'we'\n",
      " 'welcome' 'will' 'written' 'years'] \n",
      "\n",
      "Bag-of-word representation: [[0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 3 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1\n",
      "  2 0 2 0 0 1 1 0 1 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
      "  0 1 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 3 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 2 1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0\n",
      "  1 1 1 1 1 0 0 1 0 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Transform text data into Bag-of-words representation\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"Original sentences:\", sentences, \"\\n\")\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print(\"Feature names (vocabulary):\\n\", vectorizer.get_feature_names_out(), \"\\n\")\n",
    "print(\"Bag-of-word representation:\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KYgyA6hzluLg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentences: ['Welcome to COMP8420 Advanced Natural Language Processing!', 'The last few years have seen very rapid improvements in the ability of computers to understand spoken and written text, and to create and translate.', 'A lot of this improvement has come from one innovation: Transformers.', 'In this unit we will deep-dive on to the Transformer architecture, and cover applications of Transformers (and other techniques) in industry and beyond.'] \n",
      "\n",
      "Feature names (vocabulary):\n",
      " ['ability' 'advanced' 'and' 'applications' 'architecture' 'beyond' 'come'\n",
      " 'comp8420' 'computers' 'cover' 'create' 'deep' 'dive' 'few' 'from' 'has'\n",
      " 'have' 'improvement' 'improvements' 'in' 'industry' 'innovation'\n",
      " 'language' 'last' 'lot' 'natural' 'of' 'on' 'one' 'other' 'processing'\n",
      " 'rapid' 'seen' 'spoken' 'techniques' 'text' 'the' 'this' 'to'\n",
      " 'transformer' 'transformers' 'translate' 'understand' 'unit' 'very' 'we'\n",
      " 'welcome' 'will' 'written' 'years'] \n",
      "\n",
      "TF-IDF representation: [[0.         0.39505606 0.         0.         0.         0.\n",
      "  0.         0.39505606 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.39505606 0.\n",
      "  0.         0.39505606 0.         0.         0.         0.\n",
      "  0.39505606 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.25215917 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.39505606 0.\n",
      "  0.         0.        ]\n",
      " [0.1933858  0.         0.45740276 0.         0.         0.\n",
      "  0.         0.         0.1933858  0.         0.1933858  0.\n",
      "  0.         0.1933858  0.         0.         0.1933858  0.\n",
      "  0.1933858  0.15246759 0.         0.         0.         0.1933858\n",
      "  0.         0.         0.12343565 0.         0.         0.\n",
      "  0.         0.1933858  0.1933858  0.1933858  0.         0.1933858\n",
      "  0.30493517 0.         0.2468713  0.         0.         0.1933858\n",
      "  0.1933858  0.         0.1933858  0.         0.         0.\n",
      "  0.1933858  0.1933858 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33999849 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33999849 0.33999849 0.         0.33999849\n",
      "  0.         0.         0.         0.33999849 0.         0.\n",
      "  0.33999849 0.         0.21701663 0.         0.33999849 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.26805872 0.         0.         0.26805872 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.47533108 0.20096574 0.20096574 0.20096574\n",
      "  0.         0.         0.         0.20096574 0.         0.20096574\n",
      "  0.20096574 0.         0.         0.         0.         0.\n",
      "  0.         0.31688739 0.20096574 0.         0.         0.\n",
      "  0.         0.         0.12827383 0.20096574 0.         0.20096574\n",
      "  0.         0.         0.         0.         0.20096574 0.\n",
      "  0.15844369 0.15844369 0.12827383 0.20096574 0.15844369 0.\n",
      "  0.         0.20096574 0.         0.20096574 0.         0.20096574\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Transform text data into TF-IDF representation\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"Original sentences:\", sentences, \"\\n\")\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print(\"Feature names (vocabulary):\\n\", vectorizer.get_feature_names_out(), \"\\n\")\n",
    "print(\"TF-IDF representation:\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G06U1C-KpOSo"
   },
   "source": [
    "### 4. Classification Task & Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRnEZ57s0akX"
   },
   "source": [
    "A classification task aims to map specific data points to predefined labels (e.g., positive/negative, spam/not spam).\n",
    "\n",
    "In this notebook, we'll explore this concept using a classic machine learning problem: predicting passenger survival on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "AtwqnAyuRpG_"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run following code, if it is not installed.\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "eYSzAJRdj4RU"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088122ae135d464683ab03c5a7667adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/60.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea859ffff50141e4957c261feae992ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/28.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68c1cf2d6da48b49010172d95079fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a5faed879945728c23be97947f4eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500  None        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250  None        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500  None        S  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Let's first load the Titanic dataset from a platform named Hugging Face\"\"\"\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "titanic_data = load_dataset(\"Tomate/Kaggle-Titanic\")\n",
    "titanic_df = titanic_data[\"train\"].to_pandas()\n",
    "\n",
    "print(titanic_df.iloc[0].keys())\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTGXR3Z_9B3f"
   },
   "source": [
    "**Questions:**\n",
    "\n",
    ">- Q1: What features could be important for the classification task?\n",
    ">- Q2: How can we transform the data format to make it suitable for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "j93AojVf53Cp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample:\n",
      "Sex       male\n",
      "Pclass       3\n",
      "Age       22.0\n",
      "Fare      7.25\n",
      "SibSp        1\n",
      "Parch        0\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Transformed sample:\n",
      "Pclass            3\n",
      "Age            22.0\n",
      "Fare           7.25\n",
      "SibSp             1\n",
      "Parch             0\n",
      "Sex_female    False\n",
      "Sex_male       True\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Data preprocessing\"\"\"\n",
    "\n",
    "## Question: what features can we choose?\n",
    "selected_features = ['Sex', 'Pclass', 'Age','Fare','SibSp','Parch']\n",
    "\n",
    "# Convert categorical to numeric using pandas get_dummies\n",
    "df_encoded = pd.get_dummies(titanic_df[selected_features], columns=['Sex'])\n",
    "features = df_encoded.fillna(0)  # Fill missing values\n",
    "targets = titanic_df['Survived']\n",
    "\n",
    "# Show an example of transformed data\n",
    "print(\"Original sample:\")\n",
    "print(titanic_df.iloc[0][selected_features])\n",
    "print(\"\\nTransformed sample:\")\n",
    "print(features.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dY-uR_5A-F_n"
   },
   "source": [
    "Before training a classifier, we can reserve some data points as a development validation set to further evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "p7KPzNBo6pj0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 801 samples\n",
      "Development set: 90 samples\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Split data into training and development sets\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split with 90% training and 10% development data\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(features, targets, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Development set: {X_dev.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ3GJVgF_ElB"
   },
   "source": [
    "Another important point: different features may have values of varying magnitudes, so it is common to standardize the features to the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "nFtZyAEK61U0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before standardization:\n",
      "Pclass              1\n",
      "Age              61.0\n",
      "Fare          32.3208\n",
      "SibSp               0\n",
      "Parch               0\n",
      "Sex_female      False\n",
      "Sex_male         True\n",
      "Name: 625, dtype: object\n",
      "\n",
      "After standardization:\n",
      "Pclass       -1.592229\n",
      "Age           2.107043\n",
      "Fare          0.003970\n",
      "SibSp        -0.470964\n",
      "Parch        -0.466439\n",
      "Sex_female   -0.727067\n",
      "Sex_male      0.727067\n",
      "dtype: float64\n",
      "\n",
      "Mean and standard deviation of training features:\n",
      "Means: [ 2.32 23.73 32.12  0.53  0.38  0.35  0.65]\n",
      "Standard deviations: [ 0.83 17.69 50.24  1.13  0.81  0.48  0.48]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Feature Standardization\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_dev_scaled = scaler.transform(X_dev)\n",
    "\n",
    "sample_idx = 2  # Select a sample to check the details\n",
    "original_sample = X_train.iloc[sample_idx]\n",
    "scaled_sample = pd.Series(X_train_scaled[sample_idx], index=X_train.columns)\n",
    "\n",
    "print(\"Before standardization:\")\n",
    "print(original_sample)\n",
    "print(\"\\nAfter standardization:\")\n",
    "print(scaled_sample)\n",
    "print(\"\\nMean and standard deviation of training features:\")\n",
    "print(f\"Means: {np.round(scaler.mean_, 2)}\")\n",
    "print(f\"Standard deviations: {np.round(scaler.scale_, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vZSkBuXBbp8"
   },
   "source": [
    "We can then train a logistic regression model, using the processing feature data (*X_train_scales*) and labels (*y_train*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "5Z9ZPFxS64tK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Parameters:\n",
      "Weights: [[-0.78019644 -0.2527774   0.1381524  -0.33777604 -0.04478032  0.64371883\n",
      "  -0.64371883]]\n",
      "Bias: [-0.63669662]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train a logistic regression clapip install transformers\n",
    "ssifier\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "## Question: We will use fit function to train the model, but what args should be input?\n",
    "lr_model.fit(X_train_scaled,y_train)\n",
    "\n",
    "print(\"Logistic Regression Model Parameters:\")\n",
    "print(f\"Weights: {lr_model.coef_}\")\n",
    "print(f\"Bias: {lr_model.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "MlJ_d3H47AZR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features of the selected dev sample: \n",
      "Pclass            1\n",
      "Age            37.0\n",
      "Fare           53.1\n",
      "SibSp             1\n",
      "Parch             0\n",
      "Sex_female    False\n",
      "Sex_male       True\n",
      "Name: 137, dtype: object\n",
      "\n",
      "True label: 0\n",
      "Predicted label: 0\n",
      "Prediction probabilities [Not Survived, Survived]: [0.64142665 0.35857335]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Make prediction on the development set\"\"\"\n",
    "\n",
    "y_pred = lr_model.predict(X_dev_scaled)\n",
    "y_pred_proba = lr_model.predict_proba(X_dev_scaled)\n",
    "\n",
    "# Show prediction for one example\n",
    "example_idx = 10\n",
    "print(f\"Features of the selected dev sample: \\n{X_dev.iloc[example_idx]}\")\n",
    "print(f\"\\nTrue label: {y_dev.iloc[example_idx]}\")\n",
    "print(f\"Predicted label: {y_pred[example_idx]}\")\n",
    "print(f\"Prediction probabilities [Not Survived, Survived]: {y_pred_proba[example_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rrG_hZDCocE"
   },
   "source": [
    "We can get a comprehensive evaluation results using `scikit-learn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "bwa9RCXV7BEG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on development set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Survived       0.84      0.80      0.82        54\n",
      "    Survived       0.72      0.78      0.75        36\n",
      "\n",
      "    accuracy                           0.79        90\n",
      "   macro avg       0.78      0.79      0.78        90\n",
      "weighted avg       0.79      0.79      0.79        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    \"\"\"Create and print the classification report\"\"\"\n",
    "    performance = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=[\"Not Survived\", \"Survived\"]\n",
    "    )\n",
    "    print(performance)\n",
    "\n",
    "print(\"Model performance on development set:\")\n",
    "\n",
    "## Practice: input the correct variables to print the evaluation results.\n",
    "evaluate_performance(y_dev,y_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7aCyk_4W3ri"
   },
   "source": [
    "> ðŸ’¡ **About Dataset Collection & Loading:**\n",
    "> - HuggingFace is one of the convenient methods to find and load datasets. The `datasets` library provides a simple `load_dataset()` API to directly load datasets from the Hugging Face Hub. For example:\n",
    ">   ```python\n",
    ">   from datasets import load_dataset\n",
    ">   dataset = load_dataset(\"glue\", \"cola\")  # Load the CoLA dataset\n",
    ">   ```\n",
    "> - Datasets can also be loaded from local files with formats like CSV, JSON, and TSV using various methods:\n",
    ">   ```python\n",
    ">   # Read CSV using pandas\n",
    ">   import pandas as pd\n",
    ">   df = pd.read_csv(\"path/to/data.csv\")\n",
    ">   \n",
    ">   # Read JSON file\n",
    ">   import json\n",
    ">   with open(\"path/to/data.json\", \"r\") as f:\n",
    ">       data = json.load(f)\n",
    ">   \n",
    ">   # Read TSV using pandas\n",
    ">   df = pd.read_csv(\"path/to/data.tsv\", sep=\"\\t\")\n",
    ">   ```\n",
    "> - Other possible sources include [Kaggle](https://www.kaggle.com/datasets) and official dataset websites [(e.g., CoLA)](https://nyu-mll.github.io/CoLA/). You can explore any of these resources as long as you give credit to the original creators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kvz6N0d4daq"
   },
   "source": [
    "## **This workshop aims to help you:**\n",
    "\n",
    "- Set up your environment (e.g., using conda) and install required packages\n",
    "- Import and use libraries in Python programs\n",
    "- Work with text data pre-processing basics\n",
    "- Refresh the knowledge of build a logistic regression classifier"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPH9nYz81UsDpz3406AtPhk",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
